{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW2GlcN_Y3LG"
      },
      "source": [
        "# Aim of the Project\n",
        "Aim of the project is to build an intelligent conversational chatbot, Riki, that can understand complex queries from the user and intelligently respond.\n",
        "\n",
        "### Background\n",
        "R-Intelligence Inc., an AI startup, has partnered with an online chat and discussion website bluedit.io. They have an average of over 5 million active customers across the globe and more than 100,000 active chat rooms. Due to the increased traffic, they are looking at improving their user experience with a chatbot moderator, which helps them engage in a meaningful conversation and keeps them updated on trending topics, while merely chatting with Riki, a chatbot. The Artificial Intelligence-powered chat experience provides easy access to information and a host of options to the customers.\n",
        "\n",
        "### Business Requirement\n",
        "R-Intelligence Inc. has invested in Python, PySpark, and Tensorflow. Using emerging technologies of Artificial Intelligence, Machine Learning, and Natural Language Processing, Riki – the chatbot should make the whole conversation as realistic as talking to an actual human.\n",
        "The chatbot should understand that users have different intents and make it extremely simple to work around these by presenting the users with options and recommendations that best suit their needs.\n",
        "### Suggested Approach\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "yhaweC1OY3LI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, concatenate, TimeDistributed\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "65d8v8NhZhhO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqyrItr8Y3LJ"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "yudEXDDCY3LJ"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "lines = open('/content/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "conv_lines = open('/content/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5p8j1LXY3LK",
        "outputId": "2414bcbf-1546-4ea4-c534-44db0b715f2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',\n",
              " 'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',\n",
              " 'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',\n",
              " 'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',\n",
              " \"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# The sentences that we will be using to train our model.\n",
        "lines[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQGYzEHCY3LK",
        "outputId": "2c7fec1f-95a5-4120-ce4b-bab301d42f63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\",\n",
              " \"u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\"]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# The sentences' ids, which will be processed to become our input and target data.\n",
        "conv_lines[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSwmy2lrY3LK"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CY663ytyY3LK"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary to map each line's id with its text\n",
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "#     print(_line)\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-BRBlh5TY3LL"
      },
      "outputs": [],
      "source": [
        "# Create a list of all of the conversations' lines' ids.\n",
        "convs = []\n",
        "for line in conv_lines[:-1]:\n",
        "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "#     print(_line)\n",
        "    convs.append(_line.split(','))\n",
        "\n",
        "#id and conversation sample\n",
        "# for k in convs[300]:\n",
        "#     print (k, id2line[k])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UX6wotKY3LL"
      },
      "source": [
        "### Creating question inputs and answer targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iNxR38aY3LL",
        "outputId": "efb8e1cc-9609-44c9-c4ad-bef28049f18c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "221616\n",
            "221616\n"
          ]
        }
      ],
      "source": [
        "# Sort the sentences into questions (inputs) and answers (targets)\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for conv in convs:\n",
        "    for i in range(len(conv)-1):\n",
        "        # Check if the line IDs exist in the dictionary before appending\n",
        "        if conv[i] in id2line and conv[i+1] in id2line:\n",
        "            questions.append(id2line[conv[i]])\n",
        "            answers.append(id2line[conv[i+1]])\n",
        "\n",
        "# Compare lengths of questions and answers\n",
        "print(len(questions))\n",
        "print(len(answers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha6aISg2Y3LL"
      },
      "source": [
        "### Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "u57xMqFIY3LL"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    text = \" \".join(text.split())\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZLdzxE9nY3LL"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# Clean the data\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))\n",
        "\n",
        "clean_answers = []\n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4rR6aCOY3LL",
        "outputId": "5af35cad-83f3-4ce4-9d28-6f19d1690420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======Original questions and answers=======\n",
            "Not the hacking and gagging and spitting part.  Please.\n",
            "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
            "\n",
            "======Cleaned questions and answers===\n",
            "not the hacking and gagging and spitting part please\n",
            "okay then how about we try out some french cuisine saturday night\n"
          ]
        }
      ],
      "source": [
        "print(\"======Original questions and answers=======\")\n",
        "print(questions[2])\n",
        "print(answers[2])\n",
        "\n",
        "print(\"\\n======Cleaned questions and answers===\")\n",
        "print(clean_questions[2])\n",
        "print(clean_answers[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIrwKFYPY3LM"
      },
      "source": [
        "### Selecting questions and answers with appropriate length (<20 words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-UsnZGpY3LM",
        "outputId": "2605a75f-b8d7-4352-db32-04de18a9c07f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    443232.000000\n",
            "mean         10.872094\n",
            "std          12.215895\n",
            "min           0.000000\n",
            "25%           4.000000\n",
            "50%           7.000000\n",
            "75%          14.000000\n",
            "max         555.000000\n",
            "Name: counts, dtype: float64\n",
            "16.0\n",
            "19.0\n",
            "24.0\n",
            "32.0\n",
            "58.0\n"
          ]
        }
      ],
      "source": [
        "# Find the length of sentences\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "lengths = []\n",
        "for question in clean_questions:\n",
        "    lengths.append(len(question.split()))\n",
        "for answer in clean_answers:\n",
        "    lengths.append(len(answer.split()))\n",
        "# Create a dataframe so that the values can be inspected\n",
        "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
        "\n",
        "print(lengths['counts'].describe())\n",
        "\n",
        "print(np.percentile(lengths, 80))\n",
        "print(np.percentile(lengths, 85))\n",
        "print(np.percentile(lengths, 90))\n",
        "print(np.percentile(lengths, 95))\n",
        "print(np.percentile(lengths, 99))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbpQ6G31Y3LM"
      },
      "source": [
        "Let's choose length of sequences to be of maximum 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BW7WcInNY3LM"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 20000 #50000#15000 #14999 #to decide the dimension of sentence’s one-hot vector\n",
        "EMBEDDING_DIM = 100 #to decide the dimension of Word2Vec\n",
        "MAX_LEN = 20  #to unify the length of the input sentences\n",
        "NUM_SAMPLES = 60000 #60000  # Number of samples to train on.\n",
        "GLOVE_DIR = '/kaggle/input/glove-global-vectors-for-word-representation'\n",
        "EPOCHS=40 #10\n",
        "BATCH_SIZE=256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80xcyIOAY3LM",
        "outputId": "cd66fd5f-77e0-4491-ceb0-5fb5d033cf58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "138335\n",
            "138335\n",
            "nothing really but it is interesting they know each other they seem to like each other\n",
            "maybe but i think the yellow man is on drugs i think frank supplies him\n",
            "\n",
            "maybe but i think the yellow man is on drugs i think frank supplies him\n",
            "oh yeah\n",
            "\n",
            "you like mysteries that much\n",
            "yeah you are a mystery i like you very much\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Remove questions and answers that are shorter than 1 word and longer than 20 words.\n",
        "min_line_length = 2\n",
        "max_line_length = 20\n",
        "\n",
        "# Filter out the questions that are too short/long\n",
        "short_questions_temp = []\n",
        "short_answers_temp = []\n",
        "\n",
        "for i, question in enumerate(clean_questions):\n",
        "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
        "        short_questions_temp.append(question)\n",
        "        short_answers_temp.append(clean_answers[i])\n",
        "\n",
        "# Filter out the answers that are too short/long\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "\n",
        "for i, answer in enumerate(short_answers_temp):\n",
        "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
        "        short_answers.append(answer)\n",
        "        short_questions.append(short_questions_temp[i])\n",
        "\n",
        "print(len(short_questions))\n",
        "print(len(short_answers))\n",
        "\n",
        "\n",
        "r = np.random.randint(1,len(short_questions))\n",
        "for i in range(r, r+3):\n",
        "    print(short_questions[i])\n",
        "    print(short_answers[i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4ldB-ywjY3LM"
      },
      "outputs": [],
      "source": [
        "#choosing number of samples\n",
        "# encoder_input_text = clean_questions[:NUM_SAMPLES]\n",
        "encoder_input_text = short_questions[:NUM_SAMPLES]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_plkEqyIY3LM"
      },
      "source": [
        "### Put BOS tag and EOS tag for decoder input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV_1p1p3Y3LM",
        "outputId": "b1283378-f74f-4dd0-fbc4-0a0626413caa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<BOS> not the hacking and gagging and spitting part please <EOS>',\n",
              " '<BOS> okay then how about we try out some french cuisine saturday night <EOS>',\n",
              " '<BOS> forget it <EOS>',\n",
              " '<BOS> let me see what i can do <EOS>',\n",
              " '<BOS> right see you are ready for the quiz <EOS>']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "def tagger(input_text):\n",
        "  bos = \"<BOS> \"\n",
        "  eos = \" <EOS>\"\n",
        "  final_target = [bos + text + eos for text in input_text]\n",
        "  return final_target\n",
        "\n",
        "# clean_answers = clean_answers[:NUM_SAMPLES]\n",
        "# decoder_input_text = tagger(clean_answers)\n",
        "# decoder_input_text[:5]\n",
        "short_answers = short_answers[:NUM_SAMPLES]\n",
        "decoder_input_text = tagger(short_answers)\n",
        "decoder_input_text[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqUWcrPQY3LM",
        "outputId": "506b8fd6-4e2c-427e-9b39-bc8d1a36f433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "60000\n"
          ]
        }
      ],
      "source": [
        "print(len(encoder_input_text))\n",
        "print(len(decoder_input_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YOR0zntY3LM"
      },
      "source": [
        "### Make Vocabulary (VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncEkCQSWY3LM",
        "outputId": "064c9991-1f41-4eee-a86e-750940c52b6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word2idx:  {'<bos>': 1, '<eos>': 2, 'you': 3, 'i': 4, 'the': 5}\n",
            "\n",
            "idx2word:  {1: '<bos>', 2: '<eos>', 3: 'you', 4: 'i', 5: 'the'}\n",
            "word2idx length 19999\n",
            "idx2word length 19999\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words= VOCAB_SIZE, filters='')\n",
        "\n",
        "def vocab_creater(text_lists, VOCAB_SIZE):\n",
        "  tokenizer.fit_on_texts(text_lists)\n",
        "  dictionary = tokenizer.word_index\n",
        "\n",
        "  word2idx = {}\n",
        "  idx2word = {}\n",
        "  for k, v in dictionary.items():\n",
        "      if v < VOCAB_SIZE:\n",
        "          word2idx[k] = v\n",
        "          idx2word[v] = k\n",
        "      if v >= VOCAB_SIZE-1:\n",
        "          continue\n",
        "\n",
        "  return word2idx, idx2word\n",
        "\n",
        "word2idx, idx2word = vocab_creater(text_lists=encoder_input_text+decoder_input_text, VOCAB_SIZE=VOCAB_SIZE)\n",
        "\n",
        "#print first few key/value pairs\n",
        "word2idx_first5pairs = {k: word2idx[k] for k in list(word2idx)[:5]}\n",
        "print('word2idx: ', word2idx_first5pairs)\n",
        "\n",
        "idx2word_first5pairs = {k: idx2word[k] for k in list(idx2word)[:5]}\n",
        "print('\\nidx2word: ', idx2word_first5pairs)\n",
        "\n",
        "# Check the length of the dictionaries.\n",
        "print('word2idx length', len(word2idx))\n",
        "print('idx2word length', len(idx2word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftq24G5wY3LM"
      },
      "source": [
        "### Tokenize Bag of words to Bag of IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_3QtVqCY3LM",
        "outputId": "bf03a033-ebe7-47c7-c089-6e2ef7c71aa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_sequences:\n",
            " [[51, 4, 136, 19, 35, 293, 36, 17814, 54, 12, 6, 101, 36, 3], [7, 5, 9560, 18, 7938, 18, 6088, 393, 139], [3, 13, 465, 16, 48, 12, 6, 45, 941, 12, 6, 28, 159, 164], [1551, 54, 124, 19, 77, 147, 3094, 10, 1488], [17815, 991, 17816, 25, 6, 29, 354]]\n",
            "\n",
            "decoder_sequences:\n",
            " [[1, 7, 5, 9560, 18, 7938, 18, 6088, 393, 139, 2], [1, 101, 86, 42, 37, 19, 221, 48, 83, 1101, 1996, 150, 2], [1, 288, 9, 2], [1, 122, 16, 69, 14, 4, 52, 11, 2], [1, 55, 69, 3, 13, 356, 26, 5, 10775, 2]]\n"
          ]
        }
      ],
      "source": [
        "def text2seq(tokenizer, encoder_text, decoder_text, VOCAB_SIZE):\n",
        "\n",
        "#   tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
        "  encoder_sequences = tokenizer.texts_to_sequences(encoder_text)\n",
        "  decoder_sequences = tokenizer.texts_to_sequences(decoder_text)\n",
        "\n",
        "  return encoder_sequences, decoder_sequences\n",
        "\n",
        "encoder_sequences, decoder_sequences = text2seq(tokenizer, encoder_input_text, decoder_input_text, VOCAB_SIZE)\n",
        "print('encoder_sequences:\\n', encoder_sequences[:5])\n",
        "print('\\ndecoder_sequences:\\n', decoder_sequences[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F09Evvw-Y3LM"
      },
      "source": [
        "### Padding (MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ARJYxQHY3LN",
        "outputId": "1d920928-1c69-4291-a4bc-e306e058366f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_input_data:\n",
            " [[  51    4  136 ...    0    0    0]\n",
            " [   7    5 9560 ...    0    0    0]\n",
            " [   3   13  465 ...    0    0    0]\n",
            " ...\n",
            " [ 603 3243 2132 ...    0    0    0]\n",
            " [ 609   22    6 ...    0    0    0]\n",
            " [  40   22   33 ...    0    0    0]]\n",
            "\n",
            "decoder_input_data:\n",
            " [[  1   7   5 ...   0   0   0]\n",
            " [  1 101  86 ...   0   0   0]\n",
            " [  1 288   9 ...   0   0   0]\n",
            " ...\n",
            " [  1  68 113 ...   0   0   0]\n",
            " [  1  40  22 ...   0   0   0]\n",
            " [  1  40  22 ...   0   0   0]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        " from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        " def padding(encoder_sequences, decoder_sequences, MAX_LEN):\n",
        "\n",
        "  encoder_input_data = pad_sequences(encoder_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
        "  decoder_input_data = pad_sequences(decoder_sequences, maxlen=MAX_LEN, dtype='int32', padding='post', truncating='post')\n",
        "\n",
        "  return encoder_input_data, decoder_input_data\n",
        "\n",
        "encoder_input_data, decoder_input_data = padding(encoder_sequences, decoder_sequences, MAX_LEN)\n",
        "print('encoder_input_data:\\n', encoder_input_data)\n",
        "print('\\ndecoder_input_data:\\n', decoder_input_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBY2zOXDY3LN"
      },
      "source": [
        "### Reshape the Data to neural network shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6Rwx4TzQY3LN"
      },
      "outputs": [],
      "source": [
        "num_samples = len(encoder_sequences)\n",
        "\n",
        "def decoder_output_creator(decoder_input_data, num_samples, MAX_LEN, VOCAB_SIZE):\n",
        "  decoder_output_data = np.zeros((num_samples, MAX_LEN, VOCAB_SIZE), dtype=\"float32\")\n",
        "  for i, seqs in enumerate(decoder_input_data):\n",
        "      for t, seq in enumerate(seqs):\n",
        "          if t > 0:\n",
        "                decoder_output_data[i][t][seq] = 1.  #decoder_output_data[i, t, seq] = 1.\n",
        "\n",
        "  return decoder_output_data\n",
        "\n",
        "decoder_output_data = decoder_output_creator(decoder_input_data, num_samples, MAX_LEN, VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mZdkpFhY3LN",
        "outputId": "6903d669-29b3-4630-da88-c8e5a7d835d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 20)\n",
            "(60000, 20)\n",
            "(60000, 20, 20000)\n"
          ]
        }
      ],
      "source": [
        "print (encoder_input_data.shape)\n",
        "print (decoder_input_data.shape)\n",
        "print (decoder_output_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the glove model\n",
        "!wget https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip -q glove.twitter.27B.zip\n",
        "\n",
        "# Update GLOVE_DIR to the correct path\n",
        "GLOVE_DIR = '/content/'\n",
        "print(f\"GLOVE_DIR is now set to: {GLOVE_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T05-x4VSeJUZ",
        "outputId": "485634e3-aa2b-4ed4-84d3-3080e65e67db"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-19 10:30:38--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n",
            "--2025-07-19 10:30:39--  https://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip.1’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  5.05MB/s    in 4m 55s  \n",
            "\n",
            "2025-07-19 10:35:33 (4.92 MB/s) - ‘glove.twitter.27B.zip.1’ saved [1520408563/1520408563]\n",
            "\n",
            "replace glove.twitter.27B.25d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.twitter.27B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.twitter.27B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.twitter.27B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "GLOVE_DIR is now set to: /content/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9mnRdlZY3LN"
      },
      "source": [
        "### Word Embedding (EMBEDDING_DIM)\n",
        "\n",
        "We use Pretraind Word2Vec Model from Glove. We can create embedding layer with Glove with 3 steps:\n",
        "1. Call Glove file\n",
        "1. Create Embedding Matrix from our Vocabulary\n",
        "1. Create Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR_7PiseY3LN",
        "outputId": "b6cc6601-3109-4b08-bbc9-1b82b7d7eaea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1193514 word vectors.\n"
          ]
        }
      ],
      "source": [
        "# Call Glove file\n",
        "def glove_100d_dictionary(glove_dir):\n",
        "  embeddings_index = {}\n",
        "  f = open(os.path.join(glove_dir, 'glove.twitter.27B.100d.txt'))\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "  return embeddings_index\n",
        "\n",
        "embeddings_index = glove_100d_dictionary(GLOVE_DIR)\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCM7864SY3LN",
        "outputId": "0194bd15-eece-49d3-df2d-c8b3fd9d89d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 100)\n",
            "20000\n"
          ]
        }
      ],
      "source": [
        "#Create Embedding Matrix from our Vocabulary\n",
        "def embedding_matrix_creater(max_words, embedding_dimension):\n",
        "  embedding_matrix = np.zeros((max_words, embedding_dimension))  #np.zeros((len(word2idx) + 1, embedding_dimension))\n",
        "  for word, i in word2idx.items():\n",
        "        if i < max_words:\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "  return embedding_matrix\n",
        "\n",
        "embedding_matrix = embedding_matrix_creater(VOCAB_SIZE, EMBEDDING_DIM)\n",
        "print(embedding_matrix.shape)\n",
        "print((len(word2idx) + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do5NOcjUY3LN",
        "outputId": "f70e7484-c1f0-4aae-d412-66ec088e9b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#Create Embedding Layer\n",
        "def embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, MAX_LEN, embedding_matrix):\n",
        "\n",
        "  embedding_layer = Embedding(input_dim = VOCAB_SIZE,\n",
        "                              output_dim = EMBEDDING_DIM,\n",
        "                              input_length = MAX_LEN,\n",
        "                              weights = [embedding_matrix],\n",
        "                              trainable = False)\n",
        "  return embedding_layer\n",
        "\n",
        "embedding_layer = embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, MAX_LEN, embedding_matrix)\n",
        "\n",
        "embedding_layer2 = embedding_layer_creater(VOCAB_SIZE, EMBEDDING_DIM, None, embedding_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5OLboF3Y3LN"
      },
      "source": [
        "###  Split data for training validation & testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9riZAkBAY3LN"
      },
      "outputs": [],
      "source": [
        "def data_spliter(encoder_input_data, decoder_input_data, test_size1=0.2, test_size2=0.3):\n",
        "  en_train, en_test, de_train, de_test = train_test_split(encoder_input_data, decoder_input_data, test_size=test_size1)\n",
        "  en_train, en_val, de_train, de_val = train_test_split(en_train, de_train, test_size=test_size2)\n",
        "\n",
        "  return en_train, en_val, en_test, de_train, de_val, de_test\n",
        "\n",
        "en_train, en_val, en_test, de_train, de_val, de_test = data_spliter(encoder_input_data, decoder_input_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8WUUcApY3LS"
      },
      "source": [
        "# Create Seq2Seq Neural Network Architecture\n",
        "\n",
        "Seq2Seq is a type of Encoder-Decoder model using RNN. It can be used as a model for machine interaction and machine translation. By learning a large number of sequence pairs, this model generates one from the other. More kindly explained, the definition of Seq2Seq is below:\n",
        "* Input: Text Data\n",
        "* Output: Text Data as well\n",
        "\n",
        "Below is our Seq2Seq Neural Network Architecture using LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "wrHF1vmfY3LS",
        "outputId": "8eb5ff91-3adf-4a7a-92dd-375fa1ca64b4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) │  \u001b[38;5;34m2,000,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m),     │    \u001b[38;5;34m481,200\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m481,200\u001b[0m │ embedding_1[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n",
              "│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        │\n",
              "│                     │ \u001b[38;5;34m300\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m6,020,000\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m20000\u001b[0m)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">481,200</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">481,200</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">6,020,000</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,982,400\u001b[0m (34.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,982,400</span> (34.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,982,400\u001b[0m (26.64 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,982,400</span> (26.64 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,000,000\u001b[0m (7.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> (7.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, concatenate, TimeDistributed\n",
        "\n",
        "def build_seq2seq_model(HIDDEN_DIM=300):\n",
        "    #set up the encoder\n",
        "    encoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
        "    encoder_embedding = embedding_layer(encoder_inputs)\n",
        "    encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n",
        "    decoder_embedding = embedding_layer(decoder_inputs)\n",
        "    decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)\n",
        "    decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
        "    outputs = TimeDistributed(decoder_dense)(decoder_outputs)\n",
        "\n",
        "    #create seq2seq model\n",
        "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc']) #sparse_categorical_crossentropy as labels in a single integer array\n",
        "\n",
        "    #create encoder model\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    #Create sampling/decoder model\n",
        "    decoder_state_input_h  = Input(shape=(HIDDEN_DIM,))\n",
        "    decoder_state_input_c = Input(shape=(HIDDEN_DIM,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_embedding2= embedding_layer(decoder_inputs)\n",
        "    decoder_outputs2, state_h2, state_c2 = decoder_LSTM(decoder_embedding2, initial_state=decoder_states_inputs)\n",
        "    decoder_states2 = [state_h2, state_c2]\n",
        "    decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)\n",
        "\n",
        "    return model, encoder_model, decoder_model\n",
        "\n",
        "def build_seq2seq_model2(HIDDEN_DIM=300):\n",
        "    #set up the encoder\n",
        "    encoder_inputs = Input(shape=(None, ), dtype='int32',)\n",
        "    encoder_embedding = embedding_layer2(encoder_inputs)\n",
        "    encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    decoder_inputs = Input(shape=(None, ), dtype='int32',)\n",
        "    decoder_embedding = embedding_layer2(decoder_inputs)\n",
        "    decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)\n",
        "    decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
        "    outputs = TimeDistributed(decoder_dense)(decoder_outputs)\n",
        "\n",
        "    #create seq2seq model\n",
        "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc']) #sparse_categorical_crossentropy as labels in a single integer array\n",
        "\n",
        "    #create encoder model\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    #Create sampling/decoder model\n",
        "    decoder_state_input_h  = Input(shape=(HIDDEN_DIM,))\n",
        "    decoder_state_input_c = Input(shape=(HIDDEN_DIM,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_embedding2= embedding_layer2(decoder_inputs)\n",
        "    decoder_outputs2, state_h2, state_c2 = decoder_LSTM(decoder_embedding2, initial_state=decoder_states_inputs)\n",
        "    decoder_states2 = [state_h2, state_c2]\n",
        "    decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2)\n",
        "\n",
        "    return model, encoder_model, decoder_model\n",
        "\n",
        "model, encoder_model, decoder_model = build_seq2seq_model2(HIDDEN_DIM=300)\n",
        "model.summary()\n",
        "\n",
        "# model, encoder_model, decoder_model = build_seq2seq_model(HIDDEN_DIM=300)\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "wwDthpuJY3LS",
        "outputId": "73783d0b-3653-4648-bc90-3f35c0b929c3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)      │     \u001b[38;5;34m2,000,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │       \u001b[38;5;34m481,200\u001b[0m │\n",
              "│                                 │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)]     │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">481,200</span> │\n",
              "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]     │               │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,481,200\u001b[0m (9.47 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,481,200</span> (9.47 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m481,200\u001b[0m (1.84 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">481,200</span> (1.84 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,000,000\u001b[0m (7.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> (7.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "encoder_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "J2UxkrF4Y3LS",
        "outputId": "a14d3d82-eee4-4835-8b9b-a1e00cd67eb6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m) │  \u001b[38;5;34m2,000,000\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m481,200\u001b[0m │ embedding_1[\u001b[38;5;34m2\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m300\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m300\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m6,020,000\u001b[0m │ lstm_1[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│                     │ \u001b[38;5;34m20000\u001b[0m)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">481,200</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">6,020,000</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)            │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,501,200\u001b[0m (32.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,501,200</span> (32.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,501,200\u001b[0m (24.80 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,501,200</span> (24.80 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,000,000\u001b[0m (7.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> (7.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "decoder_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C10EOP95Y3LS"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMPy0ctTY3LS"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "#early stopping & saving\n",
        "# es = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
        "mcp = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_output_data,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          validation_split=0.05,\n",
        "          callbacks= [mcp]\n",
        "         )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIBn9_qSY3LS"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8dfx5zfY3LS"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "def translate_sentence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = word2idx['<bos>']\n",
        "    eos = word2idx['<eos>']\n",
        "    output_sentence = []\n",
        "\n",
        "    for _ in range(MAX_LEN):\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        # Sample a token\n",
        "        idx = np.argmax(output_tokens[0, 0, :])\n",
        "\n",
        "        if eos == idx:\n",
        "            break\n",
        "\n",
        "        word = ''\n",
        "\n",
        "        if idx > 0:\n",
        "            word = idx2word[idx]\n",
        "            output_sentence.append(word)\n",
        "\n",
        "        target_seq[0, 0] = idx\n",
        "        states_value = [h, c]  # Update states\n",
        "\n",
        "    return ' '.join(output_sentence)\n",
        "\n",
        "for index in range(10):\n",
        "    i = np.random.randint(1, len(encoder_input_data))\n",
        "    input_seq = encoder_input_data[i:i+1]\n",
        "    translation = translate_sentence(input_seq)\n",
        "    print('-')\n",
        "    print('Input:', short_answers[i])\n",
        "    print('Response:', translation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ef0262d"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}